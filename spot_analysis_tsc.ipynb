{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Spot resources Analytics\n",
    "\n",
    "Here we perform some initial process and analysis on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With static dataset, e.g. load the grabbed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse the data file and extra the results\n",
    "filename = \"subset.csv\"\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.columns = [\"info\", \"SpotPrice\", \"TimeStamp\", \"InstanceType\", \"OS type\", \"AvailabilityZone\"]\n",
    "df['TimeStamp'] =pd.to_datetime(df.TimeStamp)\n",
    "\n",
    "df.index = df.TimeStamp\n",
    "#df = df.drop('info', 1).drop(['OS type'],axis=1)\n",
    "df = df.drop(['TimeStamp'],axis=1).sort_index()\n",
    " \n",
    "\n",
    "df.head(15)\n",
    "#print (df['InstanceType'].unique())\n",
    "#print (df['AvailabilityZone'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis #2\n",
    "\n",
    "For each machine type there exists a region that is more favorable to use, as the market volatility is very low and the prices tend to stay cheaper than the other regions.\n",
    "\n",
    "With in proving this hypothesis users will be able to find the best region they should be bidding in, as long as latency is not an issue for them.\n",
    "\n",
    "Data Science tools & Techniques: We can use clustering and classification methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corrGraph(title, df):\n",
    "    corr_df = df.corr()\n",
    "    mask = np.zeros_like(corr_df)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    seaborn.heatmap(corr_df, cmap='RdYlGn_r', vmax=1.0, vmin=-1.0 , mask = mask, linewidths=2.5)\n",
    "    plt.yticks(rotation=0) \n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=90) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some info about the data\n",
    "print (df.index.min())\n",
    "print (df.index.max())\n",
    "print(df.index.max()- df.index.min()) \n",
    "df = df.truncate(before='2016-10-11', after='2016-12-12')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def awsResampler(df):\n",
    "    #RESAMPLE Data by the hour \n",
    "    dfSorted = df.groupby(['AvailabilityZone', 'InstanceType'])\n",
    "    dfSorted = dfSorted.resample('H').mean()\n",
    "    dfSorted = dfSorted.fillna(method=\"ffill\")\n",
    "    dfSorted.head(2)\n",
    "    #dfSorted=dfSorted.drop('InstanceType', axis=1).drop('AvailabilityZone', axis=3\n",
    "\n",
    "    # We have to load it into a csv to clear an issue caused by the grouping\n",
    "    # TODO investigate how to do this better for speed increase\n",
    "    dfSorted.to_csv(\"im.csv\")\n",
    "    depa = pd.read_csv(\"im.csv\")\n",
    "    depa = depa.groupby(['AvailabilityZone', 'InstanceType'])\n",
    "    return depa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "depa = awsResampler(df)\n",
    "# Initialize dictionary of all combos of dfs we want to graph and corr\n",
    "zonedfs={}\n",
    "typedfs={}\n",
    "for item in df['InstanceType'].unique():\n",
    "    typedfs.update({item: pd.DataFrame()})\n",
    "for item in df['AvailabilityZone'].unique():\n",
    "    zonedfs.update({item: pd.DataFrame()})\n",
    "\n",
    "#Fill zonedfs with dataframes of all machines in that zone pricing\n",
    "for name, group in depa:\n",
    "    if zonedfs[name[0]].empty:\n",
    "        zonedfs[name[0]] = group\n",
    "        zonedfs[name[0]] = zonedfs[name[0]].drop('InstanceType', axis=1).drop(['AvailabilityZone'],axis=1)\n",
    "        zonedfs[name[0]].rename(columns = {'SpotPrice':name[1]}, inplace = True)\n",
    "    else:\n",
    "        group1 = group.drop('InstanceType', axis=1).drop(['AvailabilityZone'],axis=1)\n",
    "        group1.rename(columns = {'SpotPrice':name[1]}, inplace = True)\n",
    "        zonedfs[name[0]] = zonedfs[name[0]].merge(group1,how='right')\n",
    "\n",
    "#Fill typedfs with dataframes of all machines in that zone pricing\n",
    "for name, group in depa:\n",
    "    if typedfs[name[1]].empty:\n",
    "        typedfs[name[1]] = group\n",
    "        typedfs[name[1]] = typedfs[name[1]].drop('InstanceType', axis=1).drop(['AvailabilityZone'],axis=1)\n",
    "        typedfs[name[1]].rename(columns = {'SpotPrice':name[0]}, inplace = True)\n",
    "    else:\n",
    "        group1 = group.drop('InstanceType', axis=1).drop(['AvailabilityZone'],axis=1)\n",
    "        group1.rename(columns = {'SpotPrice':name[0]}, inplace = True)\n",
    "        typedfs[name[1]] = typedfs[name[1]].merge(group1,how='right')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate ts\n",
    "\n",
    "df_us_west_one_a = df[df.AvailabilityZone == \"us-west-1a\"]\n",
    "df_us_west_one_b = df[df.AvailabilityZone == \"us-west-1b\"]\n",
    "\n",
    "df_us_east_one_a = df[df.AvailabilityZone == \"us-east-1a\"]\n",
    "df_us_east_one_b = df[df.AvailabilityZone == \"us-east-1b\"]\n",
    "df_us_east_one_c = df[df.AvailabilityZone == \"us-east-1c\"]\n",
    "df_us_east_one_d = df[df.AvailabilityZone == \"us-east-1d\"]\n",
    "\n",
    "df_ap_southeast_one_a = df[df.AvailabilityZone == \"ap-southeast-1a\"]\n",
    "df_ap_southeast_one_b = df[df.AvailabilityZone == \"ap-southeast-1b\"]\n",
    "\n",
    "df_ap_southeast_two_a = df[df.AvailabilityZone == \"ap-southeast-2a\"]\n",
    "df_ap_southeast_two_b = df[df.AvailabilityZone == \"ap-southeast-2b\"]\n",
    "\n",
    "#train = np.genfromtxt('datasets/train.csv', delimiter='\\t')\n",
    "#test = np.genfromtxt('datasets/test.csv', delimiter='\\t')\n",
    "\n",
    "#print(type(train))\n",
    "\n",
    "\n",
    "def get_ts_data(inst_type):\n",
    "    type_dict = {}\n",
    "    i=0\n",
    "    for dff in df_us_west_one_a,df_us_west_one_b,df_ap_southeast_two_a,df_ap_southeast_two_b, \\\n",
    "        df_us_east_one_a,df_us_east_one_b,df_us_east_one_c,df_us_east_one_d,\\\n",
    "        df_ap_southeast_one_a, df_ap_southeast_one_b:\n",
    "    \n",
    "        df2 = df[df.InstanceType == inst_type]\n",
    "        dflist = df2[\"SpotPrice\"]\n",
    "        type_dict[i] = dflist\n",
    "        #type_dict.append(dflist)\n",
    "        i = i+1\n",
    "    return type_dict\n",
    "\n",
    "c3 = get_ts_data(\"c3.large\")\n",
    "c3_x = get_ts_data(\"c3.xlarge\")\n",
    "c3_2x = get_ts_data(\"c3.2xlarge\")\n",
    "c3_4x = get_ts_data(\"c3.4xlarge\")\n",
    "c3_8x = get_ts_data(\"c3.8xlarge\")\n",
    "\n",
    "print(type(c3))\n",
    "\n",
    "print(len(c3[0]))\n",
    "ts1 = c3[0]\n",
    "ts2 = c3[1]\n",
    "ts3 = c3_8x[2]\n",
    "ts1.plot()\n",
    "ts2.plot()\n",
    "ts3.plot()\n",
    "\n",
    "plt.ylim(-2,10)\n",
    "plt.legend(['ts1','ts2','ts3'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variations = []\n",
    "for d in c3,c3_x,c3_2x,c3_4x,c3_8x:\n",
    "    for i in range(10):\n",
    "        variations.append(d[i][:12000])\n",
    "print(len(variations[0]))\n",
    "\n",
    "#variations = []\n",
    "#prices = np.array([q.open for q in quotes]).astype(np.float)\n",
    "#print(len(variations[0]))\n",
    "prices = np.array(variations)\n",
    "prices.tofile(\"prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DTW\n",
    "import time\n",
    "from math import sqrt\n",
    "\n",
    "def DTWDistance(s1, s2):\n",
    "    DTW={}\n",
    "    \n",
    "    for i in range(len(s1)):\n",
    "        DTW[(i, -1)] = float('inf')\n",
    "    for i in range(len(s2)):\n",
    "        DTW[(-1, i)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "\n",
    "    for i in range(len(s1)):\n",
    "        for j in range(len(s2)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\t\t\n",
    "    return sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "\n",
    "\n",
    "def DTWDistance(s1, s2,w):\n",
    "    DTW={}\n",
    "    \n",
    "    w = max(w, abs(len(s1)-len(s2)))\n",
    "    \n",
    "    for i in range(-1,len(s1)):\n",
    "        for j in range(-1,len(s2)):\n",
    "            DTW[(i, j)] = float('inf')\n",
    "    DTW[(-1, -1)] = 0\n",
    "  \n",
    "    for i in range(len(s1)):\n",
    "        for j in range(max(0, i-w), min(len(s2), i+w)):\n",
    "            dist= (s1[i]-s2[j])**2\n",
    "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n",
    "\t\t\n",
    "    return sqrt(DTW[len(s1)-1, len(s2)-1])\n",
    "\n",
    "def LB_Keogh(s1,s2,r):\n",
    "    LB_sum=0\n",
    "    for ind,i in enumerate(s1):\n",
    "        \n",
    "        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        \n",
    "        if i>upper_bound:\n",
    "            LB_sum=LB_sum+(i-upper_bound)**2\n",
    "        elif i<lower_bound:\n",
    "            LB_sum=LB_sum+(i-lower_bound)**2\n",
    "    \n",
    "    return sqrt(LB_sum)\n",
    "\n",
    "start_time = time.time()\n",
    "# your code\n",
    "#print(LB_Keogh(ts1, ts3, 20))\n",
    "#print(LB_Keogh(ts1[:-1],ts2[:-1],5))\n",
    "#print(time.time() - start_time , \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def knn(train,test,w):\n",
    "    preds=[]\n",
    "    for ind,i in enumerate(test):\n",
    "        min_dist=float('inf')\n",
    "        closest_seq=[]\n",
    "        #print ind\n",
    "        print(i)\n",
    "        for j in train:\n",
    "            print(i[:-1])\n",
    "            \n",
    "            print(LB_Keogh(i[:-1],j[:-1],5))\n",
    "            if LB_Keogh(i[:-1],j[:-1],5)<min_dist:\n",
    "                dist=DTWDistance(i[:-1],j[:-1],w)\n",
    "                if dist<min_dist:\n",
    "                    min_dist=dist\n",
    "                    closest_seq=j\n",
    "        preds.append(closest_seq[-1])\n",
    "    return classification_report(test[:,-1],preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = np.genfromtxt('datasets/train.csv', delimiter='\\t')\n",
    "test = np.genfromtxt('datasets/test.csv', delimiter='\\t')\n",
    "\n",
    "print(train)\n",
    "print(len(train))\n",
    "print(train[0])\n",
    "print(len(train[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = prices[:25]\n",
    "test = prices[:-25]\n",
    "len(train)\n",
    "train.tofile(\"train.csv\")\n",
    "test.tofile(\"test.csv\")\n",
    "print(train)\n",
    "print(len(train))\n",
    "print(train[0])\n",
    "print(len(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#train = np.genfromtxt('train.csv', delimiter='\\t')\n",
    "#test = np.genfromtxt('test.csv', delimiter='\\t')\n",
    "train = np.fromfile(\"train.csv\")\n",
    "test = np.fromfile(\"test.csv\")\n",
    "train2 = np.array(train)\n",
    "test2 = np.array(test)\n",
    "# your code\n",
    "#print (knn(train, test, 4))\n",
    "\n",
    "print(time.time() - start_time , \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def k_means_clust(data,num_clust,num_iter,w=5):\n",
    "    centroids=random.sample(data,num_clust)\n",
    "    counter=0\n",
    "    for n in range(num_iter):\n",
    "        counter+=1\n",
    "        print (counter)\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(data):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for c_ind,j in enumerate(centroids):\n",
    "                if LB_Keogh(i,j,5)<min_dist:\n",
    "                    cur_dist=DTWDistance(i,j,w)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "            if closest_clust in assignments:\n",
    "                assignments[closest_clust].append(ind)\n",
    "            else:\n",
    "                assignments[closest_clust]=[]\n",
    "    \n",
    "        #recalculate centroids of clusters\n",
    "        for key in assignments:\n",
    "            clust_sum=0\n",
    "            for k in assignments[key]:\n",
    "                clust_sum=clust_sum+data[k]\n",
    "            centroids[key]=[m/len(assignments[key]) for m in clust_sum]\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "train = np.genfromtxt('datasets/train.csv', delimiter='\\t')\n",
    "test = np.genfromtxt('datasets/test.csv', delimiter='\\t')\n",
    "data=np.vstack((train[:,:-1],test[:,:-1]))\n",
    "d = np.vstack(prices)\n",
    "d.tofile(\"d.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def k_means_clust(data,num_clust,num_iter,w=5):\n",
    "    centroids=random.sample(data,num_clust)\n",
    "    counter=0\n",
    "    for n in range(num_iter):\n",
    "        counter+=1\n",
    "        print (counter)\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(data):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for c_ind,j in enumerate(centroids):\n",
    "                if LB_Keogh(i,j,5)<min_dist:\n",
    "                    cur_dist=DTWDistance(i,j,w)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=c_ind\n",
    "            if closest_clust in assignments:\n",
    "                assignments[closest_clust].append(ind)\n",
    "            else:\n",
    "                assignments[closest_clust]=[]\n",
    "    \n",
    "        #recalculate centroids of clusters\n",
    "        for key in assignments:\n",
    "            clust_sum=0\n",
    "            for k in assignments[key]:\n",
    "                clust_sum=clust_sum+data[k]\n",
    "            centroids[key]=[m/len(assignments[key]) for m in clust_sum]\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "f = np.fromfile(\"prices.csv\")\n",
    "d = np.vstack(f)\n",
    "\n",
    "centroids=k_means_clust(list(d),4,10,4)\n",
    "for i in centroids:\n",
    "    \n",
    "    plt.plot(i)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in centroids:    \n",
    "#    plt.plot(i)\n",
    "#plt.show()\n",
    "\n",
    "for i in centroids:\n",
    "    print(i[0])\n",
    "    plt.plot(i[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for key in typedfs:\n",
    "    typedfs[key].index = typedfs[key].TimeStamp\n",
    "    typedfs[key]       = typedfs[key].drop(['TimeStamp'],axis=1)\n",
    "    #Normalize data\n",
    "        #typedfs[key] = typedfs[key].apply(lambda row: np.log(row).diff(), axis=0 )\n",
    "    typedfs[key] = typedfs[key].diff(axis=0)\n",
    "    corrGraph(key, typedfs[key])\n",
    "    \n",
    "for key in zonedfs:\n",
    "    zonedfs[key].index = zonedfs[key].TimeStamp\n",
    "    zonedfs[key]       = zonedfs[key].drop(['TimeStamp'],axis=1)\n",
    "    #Normalize data\n",
    "        #zonedfs[key] = zonedfs[key].apply(lambda row: np.log(row).diff(), axis=0 )\n",
    "    zonedfs[key] = zonedfs[key].diff(axis=0)\n",
    "    corrGraph(key, zonedfs[key])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  The reason for some grey rows, \n",
    "#  is because those values are all zero meaning\n",
    "#  the price did not change ever for that time series.\n",
    "\n",
    "#print(typedfs['c3.large']['sa-east-1b'].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "[pattern matching over time series data](http://stats.stackexchange.com/questions/136091/sequential-pattern-matching-in-time-series-data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
